{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n",
      "2.2.0\n",
      "12.4\n",
      "MSVC 194134120\n",
      "OrderedDict([('sys.platform', 'win32'), ('Python', '3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]'), ('CUDA available', True), ('MUSA available', False), ('numpy_random_seed', 2147483648), ('GPU 0', 'NVIDIA GeForce RTX 4080 SUPER'), ('CUDA_HOME', 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.4'), ('NVCC', 'Cuda compilation tools, release 12.4, V12.4.99'), ('MSVC', 'n/a, reason: fileno'), ('PyTorch', '2.4.1+cu124'), ('PyTorch compiling details', 'PyTorch built with:\\n  - C++ Version: 201703\\n  - MSVC 192930154\\n  - Intel(R) oneAPI Math Kernel Library Version 2024.2.1-Product Build 20240722 for Intel(R) 64 architecture applications\\n  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)\\n  - OpenMP 2019\\n  - LAPACK is enabled (usually provided by MKL)\\n  - CPU capability usage: AVX2\\n  - CUDA Runtime 12.4\\n  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\\n  - CuDNN 90.1\\n  - Magma 2.5.4\\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.4, CUDNN_VERSION=9.1.0, CXX_COMPILER=C:/actions-runner/_work/pytorch/pytorch/builder/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, \\n'), ('TorchVision', '0.19.1+cu124'), ('OpenCV', '4.10.0'), ('MMEngine', '0.10.5')])\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "import pytest\n",
    "import torch\n",
    "\n",
    "from mmaction.registry import MODELS\n",
    "from mmaction.structures import ActionDataSample\n",
    "from mmaction.testing import get_recognizer_cfg\n",
    "from mmaction.utils import register_all_modules\n",
    "from mmaction.utils.gradcam_utils import GradCAM\n",
    "import mmaction\n",
    "print(mmaction.__version__)\n",
    "\n",
    "import mmcv\n",
    "print(mmcv.__version__)\n",
    "\n",
    "    # Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())\n",
    "\n",
    "    # Check MMEngine installation\n",
    "from mmengine.utils.dl_utils import collect_env\n",
    "print(collect_env())\n",
    "\n",
    "from mmaction.apis import inference_recognizer, init_recognizer\n",
    "from mmengine import Config\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import os.path as osp\n",
    "import mmengine\n",
    "from mmengine.runner import Runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_target_shapes(input_shape, num_classes=400, model_type='2D'):\n",
    "    if model_type not in ['2D', '3D']:\n",
    "        raise ValueError(f'Data type {model_type} is not available')\n",
    "\n",
    "    preds_target_shape = (input_shape[0], num_classes)\n",
    "    if model_type == '3D':\n",
    "        # input shape (batch_size, num_crops*num_clips, C, clip_len, H, W)\n",
    "        # target shape (batch_size*num_crops*num_clips, clip_len, H, W, C)\n",
    "        blended_imgs_target_shape = (input_shape[0] * input_shape[1],\n",
    "                                     input_shape[3], input_shape[4],\n",
    "                                     input_shape[5], input_shape[2])\n",
    "    else:\n",
    "        # input shape (batch_size, num_segments, C, H, W)\n",
    "        # target shape (batch_size, num_segments, H, W, C)\n",
    "        blended_imgs_target_shape = (input_shape[0], input_shape[1],\n",
    "                                     input_shape[3], input_shape[4],\n",
    "                                     input_shape[2])\n",
    "\n",
    "    return blended_imgs_target_shape, preds_target_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _do_test_3D_models(recognizer,\n",
    "                       target_layer_name,\n",
    "                       input_shape,\n",
    "                       num_classes=400):\n",
    "    blended_imgs_target_shape, preds_target_shape = _get_target_shapes(\n",
    "        input_shape, num_classes=num_classes, model_type='3D')\n",
    "    demo_data = {\n",
    "        'inputs': [torch.randint(0, 256, input_shape[1:])],\n",
    "        'data_samples': [ActionDataSample().set_gt_label(2)]\n",
    "    }\n",
    "\n",
    "    gradcam = GradCAM(recognizer, target_layer_name)\n",
    "\n",
    "    blended_imgs, preds = gradcam(demo_data)\n",
    "    assert blended_imgs.size() == blended_imgs_target_shape\n",
    "    assert preds.size() == preds_target_shape\n",
    "\n",
    "    blended_imgs, preds = gradcam(demo_data, True)\n",
    "    assert blended_imgs.size() == blended_imgs_target_shape\n",
    "    assert preds.size() == preds_target_shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - No L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Double L_MHRA: True\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "11/21 19:03:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Drop path rate: 0.0\n",
      "Recognizer3D(\n",
      "  (data_preprocessor): ActionDataPreprocessor()\n",
      "  (backbone): UniFormerV2(\n",
      "    (conv1): Conv3d(3, 768, kernel_size=(1, 16, 16), stride=(1, 16, 16), bias=False)\n",
      "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): ModuleList(\n",
      "        (0-11): 12 x ResidualAttentionBlock(\n",
      "          (drop_path): Identity()\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dpe): ModuleList(\n",
      "        (0-3): 4 x Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=768)\n",
      "      )\n",
      "      (dec): ModuleList(\n",
      "        (0-3): 4 x Extractor(\n",
      "          (drop_path): Identity()\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): QuickGELU()\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (ln_3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  init_cfg={'type': 'Pretrained', 'checkpoint': 'https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics400/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics400-rgb_20221219-203d6aac.pth', 'prefix': 'backbone.'}\n",
      "  (cls_head): TimeSformerHead(\n",
      "    (loss_cls): CrossEntropyLoss()\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (fc_cls): Linear(in_features=768, out_features=124, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m target_layer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackbone/transformer/norm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(recognizer)\n\u001b[1;32m---> 13\u001b[0m \u001b[43m_do_test_3D_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m, in \u001b[0;36m_do_test_3D_models\u001b[1;34m(recognizer, target_layer_name, input_shape, num_classes)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_test_3D_models\u001b[39m(recognizer,\n\u001b[0;32m      2\u001b[0m                        target_layer_name,\n\u001b[0;32m      3\u001b[0m                        input_shape,\n\u001b[0;32m      4\u001b[0m                        num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     blended_imgs_target_shape, preds_target_shape \u001b[38;5;241m=\u001b[39m \u001b[43m_get_target_shapes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3D\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     demo_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m: [torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m256\u001b[39m, input_shape[\u001b[38;5;241m1\u001b[39m:])],\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: [ActionDataSample()\u001b[38;5;241m.\u001b[39mset_gt_label(\u001b[38;5;241m2\u001b[39m)]\n\u001b[0;32m     10\u001b[0m     }\n\u001b[0;32m     12\u001b[0m     gradcam \u001b[38;5;241m=\u001b[39m GradCAM(recognizer, target_layer_name)\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36m_get_target_shapes\u001b[1;34m(input_shape, num_classes, model_type)\u001b[0m\n\u001b[0;32m      5\u001b[0m preds_target_shape \u001b[38;5;241m=\u001b[39m (input_shape[\u001b[38;5;241m0\u001b[39m], num_classes)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3D\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# input shape (batch_size, num_crops*num_clips, C, clip_len, H, W)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# target shape (batch_size*num_crops*num_clips, clip_len, H, W, C)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     blended_imgs_target_shape \u001b[38;5;241m=\u001b[39m (input_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m input_shape[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     10\u001b[0m                                  input_shape[\u001b[38;5;241m3\u001b[39m], input_shape[\u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m---> 11\u001b[0m                                  \u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m, input_shape[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# input shape (batch_size, num_segments, C, H, W)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# target shape (batch_size, num_segments, H, W, C)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     blended_imgs_target_shape \u001b[38;5;241m=\u001b[39m (input_shape[\u001b[38;5;241m0\u001b[39m], input_shape[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     16\u001b[0m                                  input_shape[\u001b[38;5;241m3\u001b[39m], input_shape[\u001b[38;5;241m4\u001b[39m],\n\u001b[0;32m     17\u001b[0m                                  input_shape[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "config = get_recognizer_cfg(\n",
    "        '/Users/eddie/Trauma_THOMPSON/Trauma_THOMPSON/VideoSwin/uniformerv2_action_recognition.py')\n",
    "# config.model['backbone']['pretrained2d'] = False\n",
    "# config.model['backbone']['pretrained'] = None\n",
    "\n",
    "recognizer = init_recognizer(config=config,checkpoint=None,device='cuda:0')\n",
    "recognizer.cfg = config\n",
    "\n",
    "input_shape = (1, 3, 32, 224, 224)\n",
    "target_layer_name = 'backbone/transformer/norm'\n",
    "print(recognizer)\n",
    "\n",
    "_do_test_3D_models(recognizer, target_layer_name, input_shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
